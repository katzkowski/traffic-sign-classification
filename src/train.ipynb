{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import math\n",
    "import copy\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataset import TrafficSignDataset\n",
    "from torch.utils.data import Subset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from matplotlib.patches import Rectangle\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from torchvision import transforms\n",
    "\n",
    "from networks import TrafficSignsClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size=32\n",
    "dataset_mean = [86.72383685, 79.56345902, 81.93326525]\n",
    "dataset_std= [51.48834219, 50.93286751, 53.30977311]\n",
    "\n",
    "transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.Normalize(mean=dataset_mean, std=dataset_std)\n",
    "        ]\n",
    ")\n",
    "\n",
    "# augmentation transforms\n",
    "augmentation_transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    # transforms.GaussianBlur(kernel_size=5, sigma=(0.01, 0.6)),\n",
    "    transforms.RandomPerspective(distortion_scale=0.1, p=0.5),\n",
    "    # transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    transforms.Normalize(mean=dataset_mean, std=dataset_std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TrafficSignDataset(\"..\\data\\Train.csv\", \"..\\data\", transform=transform)\n",
    "\n",
    "# randomly split into train and validation, regardless of frame sequences\n",
    "train, val = random_split(dataset, [math.floor(len(dataset) * 0.8), math.ceil(len(dataset) * 0.2)], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train.dataset.transform = augmentation_transform\n",
    "print(len(train), len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean and variance\n",
    "# from preprocessing import calculate_mean_std\n",
    "\n",
    "# calculate_mean_std(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols, rows = 5, 6\n",
    "idx = 0 # index of frame sequence\n",
    "\n",
    "figure = plt.figure(figsize=(20, 24))\n",
    "\n",
    "for i in range(idx * 30, idx * 30 + 30):\n",
    "    img, target = train[i]\n",
    "\n",
    "    figure.add_subplot(cols, rows, i - (idx * 30) + 1)\n",
    "    # plt.imshow(img.byte().permute(1,2,0), cmap=\"gray\")\n",
    "    plt.imshow(img.byte().permute(1,2,0))\n",
    "    ax = plt.gca()\n",
    "\n",
    "    bbox = target[\"bbox\"]\n",
    "    x = bbox[2]\n",
    "    y = bbox[3]\n",
    "    box_width = bbox[4] - x\n",
    "    box_height = bbox[5] - y\n",
    "\n",
    "    rect = Rectangle((x,y), box_width, box_height, linewidth=1, edgecolor='r',facecolor='none')\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3,1, figsize=(15,15))\n",
    "\n",
    "# training data\n",
    "train_labels, train_counts = np.unique((train.dataset.targets[\"ClassId\"])[train.indices], return_counts=True)\n",
    "ax[0].bar(train_labels, train_counts / len(train) * 100)\n",
    "\n",
    "# validation data\n",
    "val_labels, val_counts = np.unique((val.dataset.targets[\"ClassId\"])[val.indices], return_counts=True)\n",
    "ax[1].bar(val_labels, val_counts, color=\"orange\")\n",
    "\n",
    "\n",
    "# val - train\n",
    "diff = val_counts / len(val) - train_counts / len(train)\n",
    "\n",
    "ax[2].bar(train_labels, diff * 100, color=np.where(diff >= 0, \"orange\",\"C0\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "# setup dataloaders\n",
    "train_dataloader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# TODO: test_dataloader\n",
    "\n",
    "# to test training setup for errors\n",
    "sample_subset = Subset(dataset, np.arange(batch_size))\n",
    "sample_dataloader =  DataLoader(sample_subset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"lr\": 1e-3,\n",
    "    \"bn_momentum\": 0.1,\n",
    "    \"momentum\": 0.9, \n",
    "    \"weight_decay\": 0,\n",
    "    \"dampening\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model\n",
    "model = TrafficSignsClassifier(hparams, input_size=32, num_classes=len(train_labels))\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# calculate model params\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=hparams[\"lr\"])\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=hparams[\"lr\"],\n",
    "    momentum=hparams[\"momentum\"],\n",
    "    weight_decay=hparams[\"weight_decay\"],\n",
    "    dampening=hparams[\"dampening\"],\n",
    "    nesterov=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new tensorboard log\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "run_name = f\"2_conv_2_k3_linear_relu_bn_maxp_b256_randp01_1e-3_val20_SGD{timestamp}\"\n",
    "writer = SummaryWriter(f\"../runs/{run_name}\")\n",
    "\n",
    "correct = 0.0   \n",
    "total = 0\n",
    "max_epochs = 30\n",
    "validate_every = 1\n",
    "\n",
    "min_val_loss = None\n",
    "patience = 0\n",
    "stopping_threshold = 5\n",
    "\n",
    "for epoch in range(0, max_epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(tqdm(train_dataloader)):\n",
    "        # move data to device\n",
    "        x, y = data\n",
    "        x = x.to(device)\n",
    "        y = y[\"label\"].to(device)\n",
    "\n",
    "        # zero param gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimizer\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # calc stats\n",
    "        running_loss += loss.item()\n",
    "        _, preds = torch.max(logits, 1)\n",
    "        correct += preds.eq(y).sum().item()\n",
    "        total += y.size(0)\n",
    "        \n",
    "    # print stats to console\n",
    "    running_loss /= len(train_dataloader)\n",
    "    correct /= total\n",
    "    print(\"[Epoch %d/%d] loss: %.3f acc: %.2f %%\" % (epoch+1, max_epochs, running_loss, 100*correct))\n",
    "\n",
    "    # log to tensorboard\n",
    "    writer.add_scalar('Training loss', running_loss, epoch * len(train_dataloader) * (batch_size/32) + i)\n",
    "    writer.add_scalar('Training acc', correct, epoch * len(train_dataloader) * (batch_size/32))\n",
    "\n",
    "    # reset stats after epoch\n",
    "    running_loss = 0.0\n",
    "    correct = 0.0\n",
    "    total = 0\n",
    "\n",
    "    # validation loop\n",
    "    if epoch % validate_every == (validate_every - 1):\n",
    "\n",
    "        val_running_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(val_dataloader):\n",
    "                # move data to device\n",
    "                x, y = data\n",
    "                x, y = x.to(device), y[\"label\"].to(device)\n",
    "                \n",
    "                # prediction + loss\n",
    "                logits = model(x)\n",
    "                val_loss = criterion(logits, y)\n",
    "                val_running_loss += val_loss.item()\n",
    "                \n",
    "                # statistics\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                val_acc += torch.sum(preds == y).item()\n",
    "                val_total += y.size(0)\n",
    "\n",
    "        # normalize loss and acc\n",
    "        val_running_loss /= len(val_dataloader)\n",
    "        val_acc /= val_total\n",
    "        print(\"Validation loss: %.3f acc: %.2f %%\" % (val_running_loss, val_acc * 100))\n",
    "\n",
    "        # log to tensorboard\n",
    "        writer.add_scalar('Validation loss', val_running_loss, epoch * len(train_dataloader) * (batch_size/32) + i)\n",
    "        writer.add_scalar('Validation acc', val_acc, epoch * len(train_dataloader) * (batch_size/32))\n",
    "    \n",
    "        # save best model and early stopping\n",
    "        if min_val_loss is None or val_running_loss < min_val_loss:\n",
    "            patience = 0\n",
    "            min_val_loss = val_running_loss\n",
    "            best_acc = val_acc\n",
    "\n",
    "            # save params\n",
    "            best_model_weights = copy.deepcopy(model.state_dict())\n",
    "        elif patience >= stopping_threshold:\n",
    "            print(\"Early stopping, best model at epoch \", epoch - stopping_threshold)\n",
    "            break\n",
    "        else:\n",
    "            patience += 1\n",
    "            continue\n",
    "\n",
    "if patience < stopping_threshold:\n",
    "    print(\"Best model at epoch \", max_epochs - patience)\n",
    "    \n",
    "print(\"Val loss: %.3f acc: %.2f %%\" % (min_val_loss, best_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best weights\n",
    "model.load_state_dict(best_model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save best model \n",
    "torch.save(model.state_dict(), f\"..\\models\\{run_name}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(train_labels)\n",
    "\n",
    "confusion_matrix = torch.zeros(num_classes, num_classes)\n",
    "with torch.no_grad():\n",
    "    for i, (x, y) in enumerate(val_dataloader):\n",
    "        x = x.to(device)\n",
    "        y = y[\"label\"].to(device)\n",
    "        outputs = model(x)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        for t, p in zip(y.view(-1), preds.view(-1)):\n",
    "                confusion_matrix[t, p] += 1\n",
    "\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "rows_sum = confusion_matrix.sum(axis=1)\n",
    "\n",
    "norm_confusion_matrix = confusion_matrix / rows_sum[:, np.newaxis]\n",
    "\n",
    "sns.set(rc={'figure.figsize':(24,20)})\n",
    "ax = sns.heatmap(norm_confusion_matrix, annot=True, cmap='Blues', fmt='.2f')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "28ba2f32f3a695c978abacd56b304aa4de162295fc3a67cb69c1cbb34fdbab11"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
